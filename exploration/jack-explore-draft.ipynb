{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"biden = pd.read_csv('/kaggle/input/us-election-2020-tweets/hashtag_joebiden.csv', lineterminator='\\n', \n                    parse_dates=True)\ntrump = pd.read_csv('/kaggle/input/us-election-2020-tweets/hashtag_donaldtrump.csv', lineterminator='\\n', \n                    parse_dates=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this project, we're interested in seeing if tweets can predict election results."},{"metadata":{},"cell_type":"markdown","source":"We're only interested in the tweets before election day (November 3, 2020)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#converted tweet_created into a datetime object and made it into a new column\nimport datetime\n\ndatetime_obj = []\n\nfor elem in biden['created_at']:\n    obj = datetime.datetime.strptime(elem, '%Y-%m-%d %H:%M:%S').date()\n    datetime_obj.append(obj)\n    \nbiden['datetime'] = datetime_obj\n\n#get all tweets before 11/3 election day\nbiden = biden[(biden['datetime'] < datetime.date(2020, 11, 3))]\nbiden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're also only interested in tweets in the U.S."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get all tweets from US\nbiden = biden[biden['country'] == 'United States of America']\nbiden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to use Vader to get the polarity score of each tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"#vader sentimenal analysis\nvader = pd.read_csv('/kaggle/input/vader-sentiment/vader_lexicon.txt', sep = '\\t', index_col = 0, \n                   header = None).drop([2,3], axis = 1).rename(columns={1: 'polarity'})\n\n#Get rid of all punctutations\npunct_re = r'[^(\\w)(\\s)]'\nbiden['no_punc'] = biden['tweet'].str.lower().replace(punct_re, ' ', regex = True) \n\n#Make new dataframe\ntidy_format = pd.DataFrame(biden['no_punc'].str.split(expand = True).stack()).reset_index(level = 1).rename(columns = {'level_1' : 'num', 0 : 'word'})\n\ntidy_format2 = tidy_format\ntidy_format2['index'] = tidy_format.index\n\n#make polarity column\nbiden['polarity_vader'] = tidy_format2.merge(vader, how = 'left', left_on = 'word', right_on = 0).fillna(0).groupby('index').sum()['polarity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're also going to use TextBlob to get the polarity score of each tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"#used TextBlob sentimental analysis library to get polarity score of every single tweet\nfrom textblob import TextBlob\n\neach_polarity = []\n\nfor elem in biden['tweet']:\n    blob = TextBlob(elem)\n    polarity = blob.sentiment.polarity\n    each_polarity.append(polarity) \n    \nbiden['polarity_textblob'] = each_polarity\nbiden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average the 2 polarity scores together."},{"metadata":{"trusted":true},"cell_type":"code","source":"biden['average_polarity'] = (biden['polarity_vader'] + biden['polarity_textblob']) / 2\nbiden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop all tweets with an average polarity score of 0 which indicates that the tweet is neutral."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get rid of tweets with polarity score of 0\nbiden = biden[biden['average_polarity'] != 0.0]\nbiden = biden.reset_index()\nbiden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a y_label which is a column with 0 (indicating they support Trump) and 1 (indicating they support biden). This label is created based on the average polarity score. Since this dataframe contains tweets about Biden, if the polarity score is positive, then we can assume the tweet supports Biden and vice versa. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 if polarity score is greater than 0 which means the tweet supports biden\n#0 if polarity score is less than 0 which means the tweet supports trump\nfor_biden = []\n\nfor polarity in biden['average_polarity']:\n    if polarity > 0:\n        for_biden.append(1)\n    else:\n        for_biden.append(0)\n\nbiden['biden_or_trump'] = for_biden\nbiden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop unnecessary columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop unnecessary columns\ndrop_cols = ['index', 'created_at', 'tweet_id', 'likes', 'retweet_count', 'source', 'user_id', 'user_name',\n            'user_screen_name', 'user_description', 'user_join_date', 'user_followers_count', 'user_location',\n            'lat', 'long', 'city', 'country', 'continent', 'state_code', 'collected_at', 'datetime']\nbiden = biden.drop(drop_cols, axis = 1)\nbiden.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Repeat these steps for Trump's dataframe."},{"metadata":{},"cell_type":"markdown","source":"Get tweets before November 3, 2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"#repeat for trump\ndatetime_obj_trump = []\n\nfor elem in trump['created_at']:\n    obj = datetime.datetime.strptime(elem, '%Y-%m-%d %H:%M:%S').date()\n    datetime_obj_trump.append(obj)\n    \n#created new column that contains datetime object\ntrump['datetime'] = datetime_obj_trump\n\n#got rows between October 20 and November 3 inclusive\ntrump = trump[(trump['datetime'] < datetime.date(2020, 11, 3))]\ntrump.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get tweets in the US only."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get all tweets from US\ntrump = trump[trump['country'] == 'United States of America']\ntrump.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get Vader polarity scores for each tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"trump['no_punc'] = trump['tweet'].str.lower().replace(punct_re, ' ', regex = True) \n\n#Make new dataframe\ntidy_format = pd.DataFrame(trump['no_punc'].str.split(expand = True).stack()).reset_index(level = 1).rename(columns = {'level_1' : 'num', 0 : 'word'})\n\ntidy_format2 = tidy_format\ntidy_format2['index'] = tidy_format.index\n\n#make polarity column\ntrump['polarity_vader'] = tidy_format2.merge(vader, how = 'left', left_on = 'word', right_on = 0).fillna(0).groupby('index').sum()['polarity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get Textblob polarity score for each tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"#used TextBlob sentimental analysis library to get polarity score of every single tweet\nfrom textblob import TextBlob\n\neach_polarity_trump = []\n\nfor elem in trump['tweet']:\n    blob = TextBlob(elem)\n    polarity = blob.sentiment.polarity\n    each_polarity_trump.append(polarity) \n    \ntrump['polarity_textblob'] = each_polarity_trump\ntrump.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average Textblob and Vader polarity scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"trump['average_polarity'] = (trump['polarity_vader'] + trump['polarity_textblob']) / 2\ntrump.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove tweets with an average polarity score of 0.0."},{"metadata":{"trusted":true},"cell_type":"code","source":"trump = trump[trump['average_polarity'] != 0.0]\ntrump = trump.reset_index()\ntrump.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create y label with 0 indicating they support Trump and 1 indicating they support Biden."},{"metadata":{"trusted":true},"cell_type":"code","source":"#0 if polarity is greater than 0, tweet supports trump which is consistent with earlier\n#1 if polarity is less than 0, tweet does not support trump \n\nfor_trump = []\n\nfor polarity in trump['average_polarity']:\n    if polarity > 0:\n        for_trump.append(0)\n    else:\n        for_trump.append(1)\n        \ntrump['biden_or_trump'] = for_trump\ntrump.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trump = trump.drop(drop_cols, axis = 1)\ntrump.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biden_and_trump = biden.append(trump, ignore_index = True)\nbiden_and_trump.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a Wordcloud for all tweets without punctuation."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = ''\n\nfor tweet in biden_and_trump['no_punc']:\n    all_words += tweet\n    all_words += ' '","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a wordcloud to see how frequent non-stop words appear\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, \n                     min_font_size = 10).generate(all_words)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a Wordcloud for all hashtags that were used in all the tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract all hashtags from every single tweet and place it into all_hashtags\nimport re\n\nhashtag = r'#[a-zA-Z]{1,}'\n\nall_hashtags = ''\nfor tweet in biden_and_trump['tweet']:\n    array_hash = re.findall(hashtag, tweet)\n    for elem in array_hash:\n        all_hashtags += elem\n        all_hashtags += ' '","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, \n                     min_font_size = 10).generate(all_hashtags)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split data into train and test data. Test data only contains tweet from Pennsylvania, Michigan, and Wisconsin. Rest is train data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get all non-swing states (train)\ntrain = biden_and_trump.copy()\ntrain = train[(train['state'] != 'Michigan') & (train['state'] != 'Pennsylvania') & (train['state'] != 'Wisconsin')]\n\n#get all swing states here (test data)\ntest = biden_and_trump.copy()\ntest = test[(test['state'] == 'Michigan') | (test['state'] == 'Pennsylvania') | (test['state'] == 'Wisconsin')]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print most negative tweets \nprint('Most negative tweets:')\nfor t in train.sort_values('average_polarity').head(10)['tweet']:\n    print('\\n  ', t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print most positive tweets \nprint('Most positive tweets:')\nfor t in train.sort_values('average_polarity', ascending = False).head(10)['tweet']:\n    print('\\n  ', t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define a function that creates a matrix where each row represents a tweet and each column is a word. This function essentially creates a one-hot encoding using words as features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#function takes in a list of (words) and a series of tweets (texts) and outputs a matrix \n#each row corresponds to a single tweet in the pandas series\n#row contains 0 or 1 for each word in the list depending if word exists\ndef words_in_texts(words, texts):\n    n = len(texts)\n    p = len(words)\n    new_array = []\n    \n    for num in range(p):\n        new_array.append(texts.str.contains(words[num], regex = False).values)\n    \n    indicator_array = np.array(new_array).T.astype(int)\n    return indicator_array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select words/hashtags from above WordCloud as features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# words = ['democracy', 'president trump', 'democrat', 'trump supporter', 'corruption', 'trump win', 'vote trump',\n#         'cnn', 'republican', 'racist']\n\nwords = ['#VoteBidenHarris2020', '#VoteBidenHarrisToSaveAmerica', 'empathetic', 'wise', 'respectful', 'intuitive', \n         'Devotion', 'Compassion', '#voteTrumpPence', '#tEAMtRUMP', '#VoteTrump', 'catastrophe', 'apocalyptic', \n         'joke', 'Lies', 'WHINE', 'SHAME', 'Failure', 'evil', 'racist', 'homophobic', 'corrupt', 'sexist', 'god', \n         'bless', 'blessed', 'jesus', 'lord', 'christ', 'sin', 'worship', 'preach', 'idiot', 'creepy', 'horrible', \n         'strange', 'racist', 'racism', 'protesters', 'transgender', 'democratic', '#VoteHimOut', '#TrumpIsLosing', \n         '#TrumpMeltdown', '#VoteRedToSaveAmerica', '#VoteRed', '#TrumpSupporters', '#HunterBidenEmails', \n         '#HunterBiden', '#BidenCrimeFamily', '#VoteBlue', '#BidenHarrisToSaveAmerica', '#RepublicansForBiden', \n         '#VoteResponsibly', '#CountEveryVote', '#RiggedElection']\n\nX_train = words_in_texts(words, train['tweet'])\nY_train = train['biden_or_trump']\n\nX_test = words_in_texts(words, test['tweet'])\nY_test = test['biden_or_trump']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train models and predict on the y_label."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_model =  LogisticRegression(max_iter = 1000)\nlog_model.fit(X_train, Y_train)\n\ntraining_accuracy = log_model.score(X_train, Y_train)\nprint(\"Training Accuracy: \", training_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracy = log_model.score(X_test, Y_test)\nprint(\"Test Accuracy: \", test_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(50, 10), activation='logistic', \n                    random_state=42, max_iter=500, solver='adam')\nmlp.fit(X_train, Y_train)\nmlp.score(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracy_mlp = mlp.score(X_test, Y_test)\nprint(\"Test Accuracy: \", test_accuracy_mlp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, our models accuracy isn't good. Instead, we will try to use our y label to determine which side the swing states will favor more. "},{"metadata":{"trusted":true},"cell_type":"code","source":"michigan = biden_and_trump[biden_and_trump['state'] == 'Michigan']\npennsylvania = biden_and_trump[biden_and_trump['state'] == 'Pennsylvania']\nwisconsin = biden_and_trump[biden_and_trump['state'] == 'Wisconsin']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_tf = michigan['biden_or_trump'].value_counts()\nm_biden = m_tf[1] / (m_tf[0] + m_tf[1])\nprint(m_biden)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_tf = pennsylvania['biden_or_trump'].value_counts()\np_biden = p_tf[1] / (p_tf[0] + p_tf[1])\nprint(p_biden)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_tf = wisconsin['biden_or_trump'].value_counts()\nw_biden = w_tf[1] / (w_tf[0] + w_tf[1])\nprint(w_biden)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that there are more tweets that favor Biden over Trump for all three states. When compared to the actual election result, the percentages do align. All three states did favor Biden over Trump by a slight margin."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}